# -*- coding: utf-8 -*-
"""Future_Gait_DLR_Net_public_data_Stair_slope_Kinematics_kinetics_Public_dataset_kinematics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116DLyfghavEPPeefrKabrJvohcuH2_xs
"""

# Let's import all packages that we may need:
# Let's import all packages that we may need:
import numpy
import tensorflow as tf
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math
import tsfel

 
import numpy as np

from scipy.signal import butter,filtfilt
 
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt
 
## for Deep-learing:
import tensorflow.keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD 
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
from keras.layers import TimeDistributed
from keras.layers import Flatten
from keras.layers import Bidirectional
#import constraint
 
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
 
 
###  Library for attention layers 
 
import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os 
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting
 
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc

import wget
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
from tqdm.notebook import tqdm

from tcn import TCN

 
### Early stopping 
 
from tensorflow.keras.callbacks import EarlyStopping

 

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.compat.v1.Session(config=config)

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())


#sess = tf.compat.v1.Session(config=tf.ConfigProto(log_device_placement=True))
#from google.colab import drive
#drive.mount('/content/drive',force_remount=True)

### subject 6 ###

#55.34

w6=74.84*9.81
h6=1.8

IMU_6= loadtxt('subject_6_treamill_IMU.csv', delimiter=',')
IK_6= loadtxt('subject_6_treamill_IK.csv', delimiter=',')
ID_6= loadtxt('subject_6_treamill_ID.csv', delimiter=',')
GRF_6= loadtxt('subject_6_treamill_GRF.csv', delimiter=',')

subject_6_treadmill=np.concatenate((IMU_6,IK_6,ID_6/(w6*h6),GRF_6/w6),axis=1)

### subject 7 ###

w7=55.34*9.81
h7=1.65

IMU_7= loadtxt('subject_7_treamill_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_treamill_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_treamill_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_treamill_GRF.csv', delimiter=',')

subject_7_treadmill=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_levelground_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_levelground_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_levelground_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_levelground_GRF.csv', delimiter=',')

subject_7_levelground=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_ramp_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_ramp_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_ramp_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_ramp_GRF.csv', delimiter=',')

subject_7_ramp=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

IMU_7= loadtxt('subject_7_stair_IMU.csv', delimiter=',')
IK_7= loadtxt('subject_7_stair_IK.csv', delimiter=',')
ID_7= loadtxt('subject_7_stair_ID.csv', delimiter=',')
GRF_7= loadtxt('subject_7_stair_GRF.csv', delimiter=',')

subject_7_stair=np.concatenate((IMU_7,IK_7,ID_7/(w7*h7),GRF_7/w7),axis=1)

### subject 8 ###

w8=72.57*9.81
h8=1.74

IMU_8= loadtxt('subject_8_treamill_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_treamill_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_treamill_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_treamill_GRF.csv', delimiter=',')

subject_8_treadmill=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_levelground_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_levelground_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_levelground_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_levelground_GRF.csv', delimiter=',')

subject_8_levelground=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_ramp_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_ramp_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_ramp_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_ramp_GRF.csv', delimiter=',')

subject_8_ramp=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)


IMU_8= loadtxt('subject_8_stair_IMU.csv', delimiter=',')
IK_8= loadtxt('subject_8_stair_IK.csv', delimiter=',')
ID_8= loadtxt('subject_8_stair_ID.csv', delimiter=',')
GRF_8= loadtxt('subject_8_stair_GRF.csv', delimiter=',')

subject_8_stair=np.concatenate((IMU_8,IK_8,ID_8/(w8*h8),GRF_8/w8),axis=1)

### subject 9 ###

w9=63.5*9.81
h9=1.63

IMU_9= loadtxt('subject_9_treamill_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_treamill_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_treamill_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_treamill_GRF.csv', delimiter=',')

subject_9_treadmill=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_levelground_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_levelground_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_levelground_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_levelground_GRF.csv', delimiter=',')

subject_9_levelground=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_1.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_1.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_1.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_1.csv', delimiter=',')

subject_9_ramp_1=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


IMU_9= loadtxt('subject_9_ramp_IMU_2.csv', delimiter=',')
IK_9= loadtxt('subject_9_ramp_IK_2.csv', delimiter=',')
ID_9= loadtxt('subject_9_ramp_ID_2.csv', delimiter=',')
GRF_9= loadtxt('subject_9_ramp_GRF_2.csv', delimiter=',')

subject_9_ramp_2=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)


subject_9_ramp=np.concatenate((subject_9_ramp_1,subject_9_ramp_2),axis=0)



IMU_9= loadtxt('subject_9_stair_IMU.csv', delimiter=',')
IK_9= loadtxt('subject_9_stair_IK.csv', delimiter=',')
ID_9= loadtxt('subject_9_stair_ID.csv', delimiter=',')
GRF_9= loadtxt('subject_9_stair_GRF.csv', delimiter=',')

subject_9_stair=np.concatenate((IMU_9,IK_9,ID_9/(w9*h9),GRF_9/w9),axis=1)

### subject 10 ###

w10=83.91*9.81
h10=1.75

IMU_10= loadtxt('subject_10_treamill_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_treamill_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_treamill_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_treamill_GRF.csv', delimiter=',')

subject_10_treadmill=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_levelground_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_levelground_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_levelground_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_levelground_GRF.csv', delimiter=',')

subject_10_levelground=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)


IMU_10= loadtxt('subject_10_ramp_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_ramp_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_ramp_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_ramp_GRF.csv', delimiter=',')

subject_10_ramp=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

IMU_10= loadtxt('subject_10_stair_IMU.csv', delimiter=',')
IK_10= loadtxt('subject_10_stair_IK.csv', delimiter=',')
ID_10= loadtxt('subject_10_stair_ID.csv', delimiter=',')
GRF_10= loadtxt('subject_10_stair_GRF.csv', delimiter=',')

subject_10_stair=np.concatenate((IMU_10,IK_10,ID_10/(w10*h10),GRF_10/w10),axis=1)

### subject 11 ###

#55.34
w11=77.11*9.81
h11=1.75

IMU_11= loadtxt('subject_11_treamill_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_treamill_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_treamill_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_treamill_GRF.csv', delimiter=',')

subject_11_treadmill=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_levelground_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_levelground_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_levelground_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_levelground_GRF.csv', delimiter=',')

subject_11_levelground=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_ramp_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_ramp_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_ramp_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_ramp_GRF.csv', delimiter=',')

subject_11_ramp=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)


IMU_11= loadtxt('subject_11_stair_IMU.csv', delimiter=',')
IK_11= loadtxt('subject_11_stair_IK.csv', delimiter=',')
ID_11= loadtxt('subject_11_stair_ID.csv', delimiter=',')
GRF_11= loadtxt('subject_11_stair_GRF.csv', delimiter=',')

subject_11_stair=np.concatenate((IMU_11,IK_11,ID_11/(w11*h11),GRF_11/w11),axis=1)

### subject 12 ###

#55.34
w12=86.18*9.81
h12=1.74

IMU_12= loadtxt('subject_12_treamill_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_treamill_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_treamill_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_treamill_GRF.csv', delimiter=',')

subject_12_treadmill=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_levelground_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_levelground_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_levelground_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_levelground_GRF.csv', delimiter=',')

subject_12_levelground=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_ramp_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_ramp_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_ramp_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_ramp_GRF.csv', delimiter=',')

subject_12_ramp=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)


IMU_12= loadtxt('subject_12_stair_IMU.csv', delimiter=',')
IK_12= loadtxt('subject_12_stair_IK.csv', delimiter=',')
ID_12= loadtxt('subject_12_stair_ID.csv', delimiter=',')
GRF_12= loadtxt('subject_12_stair_GRF.csv', delimiter=',')

subject_12_stair=np.concatenate((IMU_12,IK_12,ID_12/(w12*h12),GRF_12/w12),axis=1)

### subject 13 ###

#55.34
w13=58.97*9.81
h13=1.73

IMU_13= loadtxt('subject_13_treamill_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_treamill_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_treamill_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_treamill_GRF.csv', delimiter=',')

subject_13_treadmill=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)



IMU_13= loadtxt('subject_13_levelground_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_levelground_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_levelground_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_levelground_GRF.csv', delimiter=',')

subject_13_levelground=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_ramp_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_ramp_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_ramp_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_ramp_GRF.csv', delimiter=',')

subject_13_ramp=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)


IMU_13= loadtxt('subject_13_stair_IMU.csv', delimiter=',')
IK_13= loadtxt('subject_13_stair_IK.csv', delimiter=',')
ID_13= loadtxt('subject_13_stair_ID.csv', delimiter=',')
GRF_13= loadtxt('subject_13_stair_GRF.csv', delimiter=',')

subject_13_stair=np.concatenate((IMU_13,IK_13,ID_13/(w13*h13),GRF_13/w13),axis=1)

### subject 14 ###

#55.34
w14=58.41*9.81
h14=1.52

IMU_14= loadtxt('subject_14_treamill_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_treamill_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_treamill_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_treamill_GRF.csv', delimiter=',')

subject_14_treadmill=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)



IMU_14= loadtxt('subject_14_levelground_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_levelground_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_levelground_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_levelground_GRF.csv', delimiter=',')

subject_14_levelground=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_ramp_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_ramp_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_ramp_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_ramp_GRF.csv', delimiter=',')

subject_14_ramp=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)


IMU_14= loadtxt('subject_14_stair_IMU.csv', delimiter=',')
IK_14= loadtxt('subject_14_stair_IK.csv', delimiter=',')
ID_14= loadtxt('subject_14_stair_ID.csv', delimiter=',')
GRF_14= loadtxt('subject_14_stair_GRF.csv', delimiter=',')

subject_14_stair=np.concatenate((IMU_14,IK_14,ID_14/(w14*h14),GRF_14/w14),axis=1)

### subject 15 ###

#55.34
w15=96.16*9.81
h15=1.78

IMU_15= loadtxt('subject_15_treamill_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_treamill_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_treamill_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_treamill_GRF.csv', delimiter=',')

subject_15_treadmill=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_levelground_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_levelground_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_levelground_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_levelground_GRF.csv', delimiter=',')

subject_15_levelground=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_ramp_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_ramp_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_ramp_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_ramp_GRF.csv', delimiter=',')

subject_15_ramp=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)


IMU_15= loadtxt('subject_15_stair_IMU.csv', delimiter=',')
IK_15= loadtxt('subject_15_stair_IK.csv', delimiter=',')
ID_15= loadtxt('subject_15_stair_ID.csv', delimiter=',')
GRF_15= loadtxt('subject_15_stair_GRF.csv', delimiter=',')

subject_15_stair=np.concatenate((IMU_15,IK_15,ID_15/(w15*h15),GRF_15/w15),axis=1)

### subject 16 ###

#55.34
w16=55.79*9.81
h16=1.65

IMU_16= loadtxt('subject_16_treamill_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_treamill_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_treamill_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_treamill_GRF.csv', delimiter=',')

subject_16_treadmill=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

IMU_16= loadtxt('subject_16_levelground_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_levelground_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_levelground_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_levelground_GRF.csv', delimiter=',')

subject_16_levelground=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_ramp_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_ramp_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_ramp_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_ramp_GRF.csv', delimiter=',')

subject_16_ramp=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)


IMU_16= loadtxt('subject_16_stair_IMU.csv', delimiter=',')
IK_16= loadtxt('subject_16_stair_IK.csv', delimiter=',')
ID_16= loadtxt('subject_16_stair_ID.csv', delimiter=',')
GRF_16= loadtxt('subject_16_stair_GRF.csv', delimiter=',')

subject_16_stair=np.concatenate((IMU_16,IK_16,ID_16/(w16*h16),GRF_16/w16),axis=1)

### subject 17 ###

#55.34
w17=61.23*9.81
h17=1.68

IMU_17= loadtxt('subject_17_treamill_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_treamill_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_treamill_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_treamill_GRF.csv', delimiter=',')

subject_17_treadmill=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_levelground_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_levelground_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_levelground_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_levelground_GRF.csv', delimiter=',')

subject_17_levelground=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_ramp_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_ramp_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_ramp_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_ramp_GRF.csv', delimiter=',')

subject_17_ramp=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)


IMU_17= loadtxt('subject_17_stair_IMU.csv', delimiter=',')
IK_17= loadtxt('subject_17_stair_IK.csv', delimiter=',')
ID_17= loadtxt('subject_17_stair_ID.csv', delimiter=',')
GRF_17= loadtxt('subject_17_stair_GRF.csv', delimiter=',')

subject_17_stair=np.concatenate((IMU_17,IK_17,ID_17/(w17*h17),GRF_17/w17),axis=1)

### subject 18 ###

#55.34
w18=60.13*9.81
h18=1.8

IMU_18= loadtxt('subject_18_treamill_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_treamill_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_treamill_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_treamill_GRF.csv', delimiter=',')

subject_18_treadmill=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_levelground_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_levelground_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_levelground_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_levelground_GRF.csv', delimiter=',')

subject_18_levelground=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_ramp_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_ramp_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_ramp_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_ramp_GRF.csv', delimiter=',')

subject_18_ramp=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)


IMU_18= loadtxt('subject_18_stair_IMU.csv', delimiter=',')
IK_18= loadtxt('subject_18_stair_IK.csv', delimiter=',')
ID_18= loadtxt('subject_18_stair_ID.csv', delimiter=',')
GRF_18= loadtxt('subject_18_stair_GRF.csv', delimiter=',')

subject_18_stair=np.concatenate((IMU_18,IK_18,ID_18/(w18*h18),GRF_18/w18),axis=1)

### subject 19 ###

#55.34
w19=68.04*9.81
h19=1.7

IMU_19= loadtxt('subject_19_treamill_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_treamill_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_treamill_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_treamill_GRF.csv', delimiter=',')

subject_19_treadmill=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_levelground_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_levelground_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_levelground_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_levelground_GRF.csv', delimiter=',')

subject_19_levelground=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_ramp_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_ramp_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_ramp_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_ramp_GRF.csv', delimiter=',')

subject_19_ramp=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)


IMU_19= loadtxt('subject_19_stair_IMU.csv', delimiter=',')
IK_19= loadtxt('subject_19_stair_IK.csv', delimiter=',')
ID_19= loadtxt('subject_19_stair_ID.csv', delimiter=',')
GRF_19= loadtxt('subject_19_stair_GRF.csv', delimiter=',')

subject_19_stair=np.concatenate((IMU_19,IK_19,ID_19/(w19*h19),GRF_19/w19),axis=1)

### subject 20 ###

#55.34
w20=68.04*9.81
h20=1.71

IMU_20= loadtxt('subject_20_treamill_IMU.csv', delimiter=',')
IK_20= loadtxt('subject_20_treamill_IK.csv', delimiter=',')
ID_20= loadtxt('subject_20_treamill_ID.csv', delimiter=',')
GRF_20= loadtxt('subject_20_treamill_GRF.csv', delimiter=',')

subject_20_treadmill=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_levelground_GRF.csv', delimiter=',')

# subject_20_levelground=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_ramp_GRF.csv', delimiter=',')

# subject_20_ramp=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)


# IMU_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IMU.csv', delimiter=',')
# IK_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_IK.csv', delimiter=',')
# ID_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_ID.csv', delimiter=',')
# GRF_20= loadtxt('/content/drive/My Drive/kinematics and kinetics prediction/subject_20_stair_GRF.csv', delimiter=',')

# subject_20_stair=np.concatenate((IMU_20,IK_20,ID_20/(w20*h20),GRF_20/w20),axis=1)

### subject 21 ###

#55.34
w21=58.06*9.81
h21=1.57

IMU_21= loadtxt('subject_21_treamill_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_treamill_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_treamill_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_treamill_GRF.csv', delimiter=',')

subject_21_treadmill=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_levelground_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_levelground_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_levelground_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_levelground_GRF.csv', delimiter=',')

subject_21_levelground=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_ramp_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_ramp_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_ramp_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_ramp_GRF.csv', delimiter=',')

subject_21_ramp=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)


IMU_21= loadtxt('subject_21_stair_IMU.csv', delimiter=',')
IK_21= loadtxt('subject_21_stair_IK.csv', delimiter=',')
ID_21= loadtxt('subject_21_stair_ID.csv', delimiter=',')
GRF_21= loadtxt('subject_21_stair_GRF.csv', delimiter=',')

subject_21_stair=np.concatenate((IMU_21,IK_21,ID_21/(w21*h21),GRF_21/w21),axis=1)

### subject 23 ###

#55.34
w23=76.82*9.81
h23=1.8

IMU_23= loadtxt('subject_23_treamill_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_treamill_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_treamill_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_treamill_GRF.csv', delimiter=',')

subject_23_treadmill=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_levelground_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_levelground_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_levelground_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_levelground_GRF.csv', delimiter=',')

subject_23_levelground=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_ramp_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_ramp_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_ramp_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_ramp_GRF.csv', delimiter=',')

subject_23_ramp=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)


IMU_23= loadtxt('subject_23_stair_IMU.csv', delimiter=',')
IK_23= loadtxt('subject_23_stair_IK.csv', delimiter=',')
ID_23= loadtxt('subject_23_stair_ID.csv', delimiter=',')
GRF_23= loadtxt('subject_23_stair_GRF.csv', delimiter=',')

subject_23_stair=np.concatenate((IMU_23,IK_23,ID_23/(w23*h23),GRF_23/w23),axis=1)

### subject 24 ###

#55.34
w24=72.57*9.81
h24=1.73

IMU_24= loadtxt('subject_24_treamill_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_treamill_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_treamill_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_treamill_GRF.csv', delimiter=',')

subject_24_treadmill=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_levelground_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_levelground_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_levelground_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_levelground_GRF.csv', delimiter=',')

subject_24_levelground=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_ramp_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_ramp_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_ramp_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_ramp_GRF.csv', delimiter=',')

subject_24_ramp=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)


IMU_24= loadtxt('subject_24_stair_IMU.csv', delimiter=',')
IK_24= loadtxt('subject_24_stair_IK.csv', delimiter=',')
ID_24= loadtxt('subject_24_stair_ID.csv', delimiter=',')
GRF_24= loadtxt('subject_24_stair_GRF.csv', delimiter=',')

subject_24_stair=np.concatenate((IMU_24,IK_24,ID_24/(w24*h24),GRF_24/w24),axis=1)

### subject 25 ###

#55.34
w25=52.16*9.81
h25=1.63

IMU_25= loadtxt('subject_25_treamill_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_treamill_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_treamill_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_treamill_GRF.csv', delimiter=',')

subject_25_treadmill=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

IMU_25= loadtxt('subject_25_levelground_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_levelground_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_levelground_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_levelground_GRF.csv', delimiter=',')

subject_25_levelground=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_ramp_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_ramp_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_ramp_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_ramp_GRF.csv', delimiter=',')

subject_25_ramp=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)


IMU_25= loadtxt('subject_25_stair_IMU.csv', delimiter=',')
IK_25= loadtxt('subject_25_stair_IK.csv', delimiter=',')
ID_25= loadtxt('subject_25_stair_ID.csv', delimiter=',')
GRF_25= loadtxt('subject_25_stair_GRF.csv', delimiter=',')

subject_25_stair=np.concatenate((IMU_25,IK_25,ID_25/(w25*h25),GRF_25/w25),axis=1)

### subject 27 ###

#55.34
w27=68.04*9.81
h27=1.7

IMU_27= loadtxt('subject_27_treamill_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_treamill_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_treamill_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_treamill_GRF.csv', delimiter=',')

subject_27_treadmill=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_levelground_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_levelground_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_levelground_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_levelground_GRF.csv', delimiter=',')

subject_27_levelground=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_ramp_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_ramp_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_ramp_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_ramp_GRF.csv', delimiter=',')

subject_27_ramp=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)


IMU_27= loadtxt('subject_27_stair_IMU.csv', delimiter=',')
IK_27= loadtxt('subject_27_stair_IK.csv', delimiter=',')
ID_27= loadtxt('subject_27_stair_ID.csv', delimiter=',')
GRF_27= loadtxt('subject_27_stair_GRF.csv', delimiter=',')

subject_27_stair=np.concatenate((IMU_27,IK_27,ID_27/(w27*h27),GRF_27/w27),axis=1)

### subject 28 ###

#55.34
w28=62.14*9.81
h28=1.69

IMU_28= loadtxt('subject_28_treamill_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_treamill_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_treamill_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_treamill_GRF.csv', delimiter=',')

subject_28_treadmill=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_levelground_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_levelground_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_levelground_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_levelground_GRF.csv', delimiter=',')

subject_28_levelground=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_ramp_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_ramp_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_ramp_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_ramp_GRF.csv', delimiter=',')

subject_28_ramp=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)


IMU_28= loadtxt('subject_28_stair_IMU.csv', delimiter=',')
IK_28= loadtxt('subject_28_stair_IK.csv', delimiter=',')
ID_28= loadtxt('subject_28_stair_ID.csv', delimiter=',')
GRF_28= loadtxt('subject_28_stair_GRF.csv', delimiter=',')

subject_28_stair=np.concatenate((IMU_28,IK_28,ID_28/(w28*h28),GRF_28/w28),axis=1)

### subject 30 ###

#55.34
w30=77.03*9.81
h30=1.77

IMU_30= loadtxt('subject_30_treamill_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_treamill_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_treamill_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_treamill_GRF.csv', delimiter=',')

subject_30_treadmill=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)



IMU_30= loadtxt('subject_30_levelground_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_levelground_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_levelground_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_levelground_GRF.csv', delimiter=',')

subject_30_levelground=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_ramp_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_ramp_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_ramp_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_ramp_GRF.csv', delimiter=',')

subject_30_ramp=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)


IMU_30= loadtxt('subject_30_stair_IMU.csv', delimiter=',')
IK_30= loadtxt('subject_30_stair_IK.csv', delimiter=',')
ID_30= loadtxt('subject_30_stair_ID.csv', delimiter=',')
GRF_30= loadtxt('subject_30_stair_GRF.csv', delimiter=',')

subject_30_stair=np.concatenate((IMU_30,IK_30,ID_30/(w30*h30),GRF_30/w30),axis=1)

gc.collect()


######################################################################################################################################################################################################################################
######################################################################################################################################################################################################################################






train_dataset_treadmill=np.concatenate((subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,subject_10_treadmill,subject_11_treadmill,subject_12_treadmill,
                              subject_13_treadmill,subject_14_treadmill,subject_15_treadmill,\
                              subject_16_treadmill,subject_17_treadmill,subject_18_treadmill,subject_19_treadmill,subject_21_treadmill,\
                              subject_23_treadmill,subject_25_treadmill,subject_27_treadmill,subject_28_treadmill,subject_30_treadmill),axis=0)


train_dataset_levelground=np.concatenate((subject_7_levelground,subject_8_levelground,subject_9_levelground,subject_10_levelground,subject_11_levelground,subject_12_levelground,
                              subject_13_levelground,subject_14_levelground,subject_15_levelground,\
                              subject_16_levelground,subject_17_levelground,subject_18_levelground,subject_19_levelground,subject_21_levelground,\
                              subject_23_levelground,subject_25_levelground,subject_27_levelground,subject_28_levelground,subject_30_levelground),axis=0)


train_dataset_ramp=np.concatenate((subject_7_ramp,subject_8_ramp,subject_9_ramp,subject_10_ramp,subject_11_ramp,subject_12_ramp,
                              subject_13_ramp,subject_14_ramp,subject_15_ramp,\
                              subject_16_ramp,subject_17_ramp,subject_18_ramp,subject_19_ramp,subject_21_ramp,\
                              subject_23_ramp,subject_25_ramp,subject_27_ramp,subject_28_ramp,subject_30_ramp),axis=0)


train_dataset_stair=np.concatenate((subject_7_stair,subject_8_stair,subject_9_stair,subject_10_stair,subject_11_stair,subject_12_stair,
                              subject_13_stair,subject_14_stair,subject_15_stair,\
                              subject_16_stair,subject_17_stair,subject_18_stair,subject_19_stair,subject_21_stair,\
                              subject_23_stair,subject_25_stair,subject_27_stair,subject_28_stair,subject_30_stair),axis=0)


train_dataset=np.concatenate((train_dataset_treadmill,train_dataset_levelground,train_dataset_ramp,train_dataset_stair),axis=0)


test_dataset=np.concatenate((subject_24_treadmill,subject_24_levelground,subject_24_ramp,subject_24_stair),axis=0)


import os 
 
main_dir = "/home/sanzidpr/Chase_conference_WA2/Subject24"
os.mkdir(main_dir) 

path='/home/sanzidpr/Chase_conference_WA2/Subject24/'


subject='Subject_24'








# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
# Train features #
train_1=train_dataset[:,1:7]
train_2=train_dataset[:,7:13]
train_3=train_dataset[:,13:19]
train_4=train_dataset[:,19:25]


from sklearn.preprocessing import StandardScaler


x_train=train_1
#x_train=np.concatenate((train_3,train_2,train_1),axis=1)
scale= StandardScaler()
 
scaler = MinMaxScaler(feature_range=(0, 1))

train_X_1_1=x_train
 
 
 
 
# # Test features #
 
test_1=test_dataset[:,1:7]
test_2=test_dataset[:,7:13]
test_3=test_dataset[:,13:19]
test_4=test_dataset[:,19:25]



#    ### Extra features  ###
  
 
x_test=test_1

#x_test=np.concatenate((test_3,test_2,test_1),axis=1)

 
test_X_1_1=x_test

 


  ### Label ###
    
f=0
 


y_1_1=train_dataset[:,(f+32):(f+33)]
y_1_2=train_dataset[:,(f+35):(f+37)]
y_1_3=train_dataset[:,(f+56):(f+57)]
y_1_4=train_dataset[:,(f+65):(f+66)]
y_1_5=train_dataset[:,(f+67):(f+68)]
y_1_6=train_dataset[:,(f+74):(f+77)]
y_1_7=train_dataset[:,(f+80):(f+83)]


#train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5,y_1_6),axis=1)

train_y_1_1=np.concatenate((y_1_3,y_1_4,y_1_5),axis=1)

# train_y_1_1=y_1_4


y_2_1=test_dataset[:,(f+32):(f+33)]
y_2_2=test_dataset[:,(f+35):(f+37)]
y_2_3=test_dataset[:,(f+56):(f+57)]
y_2_4=test_dataset[:,(f+65):(f+66)]
y_2_5=test_dataset[:,(f+67):(f+68)]
y_2_6=test_dataset[:,(f+74):(f+77)]
y_2_7=test_dataset[:,(f+80):(f+83)]



 
#test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5,y_2_6),axis=1)

test_y_1_1= np.concatenate((y_2_3,y_2_4,y_2_5),axis=1)

train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)
test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)

train_dataset_1=pd.DataFrame(train_dataset_1)
test_dataset_1=pd.DataFrame(test_dataset_1)

train_dataset_1.dropna(axis=0,inplace=True)
test_dataset_1.dropna(axis=0,inplace=True)

train_dataset_1=np.array(train_dataset_1)
test_dataset_1=np.array(test_dataset_1)

train_dataset_sum = np. sum(train_dataset_1)
array_has_nan = np. isnan(train_dataset_sum)

print(array_has_nan)

print(train_dataset_1.shape)



#train_X_1=train_dataset_1[:,0:18]
#test_X_1=test_dataset_1[:,0:18]
#
#train_y_1=train_dataset_1[:,18:24]
#test_y_1=test_dataset_1[:,18:24]

train_X_1=train_dataset_1[:,0:6]
test_X_1=test_dataset_1[:,0:6]

train_y_1=train_dataset_1[:,6:9]
test_y_1=test_dataset_1[:,6:9]




L1=len(train_X_1)
L2=len(test_X_1)
# L3=len(validation_X_1)

print(L1+L2)
 
w=100

                   

 
 
a1=L1//w
b1=L1%w
 
a2=L2//w
b2=L2%w

# a3=L3//w
# b3=L3%w 
 
     #### Features ####
train_X_2=train_X_1[L1-w+b1:L1,:]
test_X_2=test_X_1[L2-w+b2:L2,:]
# validation_X_2=validation_X_1[L3-w+b3:L3,:]
 

    #### Output ####
 
train_y_2=train_y_1[L1-w+b1:L1,:]
test_y_2=test_y_1[L2-w+b2:L2,:]
# validation_y_2=validation_y_1[L3-w+b3:L3,:]


 
     #### Features ####
    
train_X=np.concatenate((train_X_1,train_X_2),axis=0)
test_X=np.concatenate((test_X_1,test_X_2),axis=0)
# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)
 
 
    #### Output ####
    
train_y=np.concatenate((train_y_1,train_y_2),axis=0)
test_y=np.concatenate((test_y_1,test_y_2),axis=0)
# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)

    
print(train_y.shape) 
    #### Reshaping ####
train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))
test_X = test_X.reshape((a2+1,w,test_X.shape[1]))


train_y_3_p= train_y.reshape((a1+1,w,3))
test_y= test_y.reshape((a2+1,w,3))
# Y_validation= validation_y.reshape((a3+1,w,6))

 

# train_X_1D=train_X_3
test_X_1D=test_X

train_X_3=train_X_3_p
train_y_3=train_y_3_p
# print(train_X_4.shape,train_y_3.shape)

train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)
#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

features=6
sensors=1


train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,sensors)
test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,sensors)
X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,sensors)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

import tensorflow as tf
# tensorflow import keras
from tensorflow.keras import layers

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()
gc.collect()




"""# Function of Base Models"""


#########################################################################################################################################################################  
####################################################   10. Kinetics-FM-Net   ####################################################################

def Kinetics_FM_Net(inputs_1D_N,inputs_2D_N):

#  model_1=Bidirectional(GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
#  model_1=Dropout(0.35)(model_1)
#  model_1=Bidirectional(GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
#  model_1=Dropout(0.35)(model_1)
  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_1=Dropout(0.1)(model_1)
  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)
  model_1=Dropout(0.1)(model_1)
#  model_1=Dense(64, activation='relu')(model_1)
#  model_1=Dropout(0.3)(model_1)
#  model_1=Dense(32,activation='relu')(model_1)
#  Model_1=Dropout(0.3)(model_1)
  model_1=Flatten()(model_1)
  
  
  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_2=Dropout(0.1)(model_2)
  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)
  model_2=Dropout(0.1)(model_2)
  model_2=Flatten()(model_2)


  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)
  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 1))(X)

  X=Dense(64, activation='relu')(X)
  X=Dropout(0.25)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.25)(X)

  X=Flatten()(X)
  X=concatenate([X,model_2])
  
  
  
  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)
  model_3=Dropout(0.1)(model_3)
  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)
  model_3=Dropout(0.1)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)

  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.25)(CNN)
  CNN=Flatten()(CNN)
  CNN=concatenate([CNN,model_3])

  output_GRU=Dense(3*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_GRU=Reshape(target_shape=(w,3))(output_GRU)
  output_C2=Dense(3*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C2=Reshape(target_shape=(w,3))(output_C2)
  output_C1=Dense(3*w,bias_regularizer=l2(0.001), activation='linear')(CNN)
  output_C1=Reshape(target_shape=(w,3))(output_C1)
  
#  output_GRU_1=Dense(128,activation='relu')(output_GRU)
#  output_GRU_1=Dense(3,activation='sigmoid')(output_GRU_1)
#  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
#  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
#  
#  output_C2_1=Dense(128,activation='relu')(output_C2)
#  output_C2_1=Dense(3,activation='sigmoid')(output_C2_1)
#  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
#  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
#
#  output_C1_1=Dense(128,activation='relu')(output_C1)
#  output_C1_1=Dense(3,activation='sigmoid')(output_C1_1)
#  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
#  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  
  
  output_GRU_1=GRU(128,return_sequences=True)(output_GRU)
  output_GRU_1=GRU(64,return_sequences=True)(output_GRU_1)
  output_GRU_1=Dense(3,activation='sigmoid')(output_GRU_1)
  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  
  output_C2_1=GRU(128,return_sequences=True)(output_C2)
  output_C2_1=GRU(64,return_sequences=True)(output_C2_1)
  output_C2_1=Dense(3,activation='sigmoid')(output_C2_1)
  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])

  output_C1_1=GRU(128,return_sequences=True)(output_C1)
  output_C1_1=GRU(64,return_sequences=True)(output_C1_1)
  output_C1_1=Dense(3,activation='sigmoid')(output_C1_1)
  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  

  weight=output_GRU_1+output_C2_1+output_C1_1
  
  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])
  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])
  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])
  

  output = output_GRU_2+output_C1_2+output_C2_2
  
  #output = Average()([output_GRU,output_C2,output_C1])
  

  return (output_C2,output_GRU,output_C1,output)

  

  
##########################################################################################################################################################################################  
##########################################################################################################################################################################################    

##########################################################################################################################################################################################  
##########################################################################################################################################################################################    


  
  
  
from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import pickle
from sklearn.linear_model import Ridge
from sklearn.utils import resample

"""# Loss Function"""

def correlation_coefficient_loss_joint_1(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)
    #l2=-K.square(r)

    l=l2+8*l1
    return l
    

def correlation_coefficient_loss_joint_2(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)
    #l2=-K.square(r)

    l=l2+16*l1
    return l
    

def correlation_coefficient_loss_joint_3(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)
    #l2=-K.square(r)

    l=l2+24*l1
    return l


def correlation_coefficient_loss_joint_4(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)
    #l2=-K.square(r)

    l=l2+4*l1
    return l
    
def correlation_coefficient_loss_joint_5(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)
    #l2=-K.square(r)

    l=l2+(8/3)*l1
    return l
    

#####################################################################################################################################################################################################################################
###########################################################################################################################################################################################
###########################################################################################################################################################################################
def prediction_test(yhat_4,test_y):

    test_o=test_y.reshape((test_y.shape[0]*w,3))
    yhat=yhat_4.reshape((test_y.shape[0]*w,3))

    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]

    
    cutoff=6
    fs=100
    order=2
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
    y_1= butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2= butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3= butter_lowpass_filter(y_3_no, cutoff, fs, order)

    
    ###calculate RMSE
    
    rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100
    rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100
    rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100
    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]

    p=np.array([p_1,p_2,p_3])

    rmse=np.array([rmse_1,rmse_2,rmse_3])
    
    m=statistics.mean(rmse)
    SD=statistics.stdev(rmse)
    print('Mean: %.3f' % m,'+/- %.3f' %SD)

    m_c=statistics.mean(p)
    SD_c=statistics.stdev(p)
    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)
    
    return rmse,p



########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################


epoch=40





#### Kinetics-FM-JL-NET ###

inputs_1D = tf.keras.layers.Input( shape=(w,6) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,1) )

inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model_3.compile(loss=correlation_coefficient_loss_joint_1, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_1])



history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=epoch, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

model_3.save(path+'model_Kinetics_FM_JL_1.h5')

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])

rmse,p= prediction_test(yhat_4, test_y)

print(rmse)
print(p)

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p


ablation_1=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])




import gc
gc.collect()
gc.collect()
gc.collect()


########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################





#### Kinetics-FM-JL-NET ###

inputs_1D = tf.keras.layers.Input( shape=(w,6) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,1) )

inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model_3.compile(loss=correlation_coefficient_loss_joint_2, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_2])



history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=epoch, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

model_3.save(path+'model_Kinetics_FM_JL_2.h5')

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])

rmse,p= prediction_test(yhat_4, test_y)

print(rmse)
print(p)

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p


ablation_2=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])




import gc
gc.collect()
gc.collect()
gc.collect()


########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################


#### Kinetics-FM-JL-NET ###

inputs_1D = tf.keras.layers.Input( shape=(w,6) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,1) )

inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model_3.compile(loss=correlation_coefficient_loss_joint_3, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_3])



history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=epoch, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

model_3.save(path+'model_Kinetics_FM_JL_3.h5')

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])

rmse,p= prediction_test(yhat_4, test_y)

print(rmse)
print(p)

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p


ablation_3=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])




import gc
gc.collect()
gc.collect()
gc.collect()


########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################




#### Kinetics-FM-JL-NET ###

inputs_1D = tf.keras.layers.Input( shape=(w,6) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,1) )

inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model_3.compile(loss=correlation_coefficient_loss_joint_4, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_4])



history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=epoch, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

model_3.save(path+'model_Kinetics_FM_JL_4.h5')

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])

rmse,p= prediction_test(yhat_4, test_y)

print(rmse)
print(p)

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p


ablation_4=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])




import gc
gc.collect()
gc.collect()
gc.collect()



########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################





#### Kinetics-FM-JL-NET ###

inputs_1D = tf.keras.layers.Input( shape=(w,6) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,1) )

inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)

model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])

opt = tf.keras.optimizers.Adam(learning_rate=3e-4)

model_3.compile(loss=correlation_coefficient_loss_joint_5, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_5])



history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=epoch, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \
                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

model_3.save(path+'model_Kinetics_FM_JL_5.h5')

[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])

rmse,p= prediction_test(yhat_4, test_y)

print(rmse)
print(p)

RMSE_Kinetics_FM_JL=rmse
PCC_Kinetics_FM_JL=p


ablation_5=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])


Kinetics_FM_JL_result=np.vstack([ablation_1,ablation_2,ablation_3,ablation_4,ablation_5])


from numpy import savetxt
savetxt(path+subject+'_all_results.csv', Kinetics_FM_JL_result, delimiter=',')


import gc
gc.collect()
gc.collect()
gc.collect()



########################################################################################################################################################################################################################
########################################################################################################################################################################################################################
########################################################################################################################################################################################################################



